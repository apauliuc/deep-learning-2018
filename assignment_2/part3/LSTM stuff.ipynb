{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from model import *\n",
    "from dataset import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'good models without Temperature/grimms_fairy_tails/best_checkpoint.pth.tar'\n",
      "Initialize dataset with 540241 characters, 87 unique.\n",
      "=> loaded checkpoint 'good models without Temperature/grimms_fairy_tails/best_checkpoint.pth.tar' (epoch 382494) with best accuracy: 0.6958\n"
     ]
    }
   ],
   "source": [
    "file = 'good models without Temperature/grimms_fairy_tails/best_checkpoint.pth.tar'\n",
    "vocab_size = 87\n",
    "seq_length = 30\n",
    "\n",
    "print(\"=> loading checkpoint '{}'\".format(file))\n",
    "checkpoint = torch.load(file, map_location='cpu')\n",
    "\n",
    "dataset = TextDataset(filename='rsc/book_EN_grimms_fairy_tails.txt', seq_length=30, batch_size=128,\n",
    "                      train_steps=100)\n",
    "model = TextGenerationModel(vocabulary_size=vocab_size, lstm_num_hidden=128,\n",
    "                            lstm_num_layers=2, device='cpu',\n",
    "                            dropout_prob=0.2)\n",
    "model.eval()\n",
    "optimizer = torch.optim.RMSprop(model.parameters())\n",
    "\n",
    "best_accuracy = checkpoint['best_accuracy']\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "print(\"=> loaded checkpoint '{}' (epoch {}) with best accuracy: {:.4f}\".format(file, checkpoint['step'], best_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print sampled data\n",
    "# with open('good models without Temperature/grimms_fairy_tails/generated_text.txt', 'r') as f:\n",
    "#     content = f.readlines()\n",
    "# content = [x.strip() for x in content[::2]]\n",
    "# content[::25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence(model, dataset, length=30):\n",
    "    with torch.no_grad():\n",
    "        sequence = Variable(torch.empty(1, 1)).long()\n",
    "        sequence.random_(to=dataset.vocab_size)\n",
    "\n",
    "        for t in range(0, seq_length):\n",
    "            out_seq, _, _ = model(sequence)\n",
    "            next_char = torch.tensor([[torch.argmax(out_seq, dim=2)[0, t]]]).long()\n",
    "            sequence = torch.cat((sequence, next_char), dim=1)\n",
    "\n",
    "        generated_sequence = dataset.convert_to_string(sequence.detach().cpu().numpy().squeeze())\n",
    "    \n",
    "    return generated_sequence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_new_seq_given_seq(model, dataset, sentence=\"The sleeping beauty is\", length=30):\n",
    "    with torch.no_grad():\n",
    "        in_seq = torch.tensor([[dataset._char_to_ix[sentence[-1]]]])\n",
    "        sentence = sentence[:len(sentence)-1]\n",
    "        sequence = torch.tensor([[dataset._char_to_ix[c] for c in sentence]])\n",
    "        init_out, h, c = model(sequence)        \n",
    "        hc_0 = tuple((h, c))\n",
    "        \n",
    "        for t in range(0, length):\n",
    "            out, h, c = model(in_seq, hc_0)\n",
    "            next_char = torch.tensor([[torch.argmax(out, dim=2)[0, t]]])\n",
    "            in_seq = torch.cat((in_seq, next_char), dim=1)\n",
    "            \n",
    "        generated_seq = dataset.convert_to_string(in_seq.detach().numpy().squeeze())\n",
    "        print(sentence + generated_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sleeping beauty is to be mayor.’ The man went out together, and the second son\n"
     ]
    }
   ],
   "source": [
    "predict_new_seq_given_seq(model, dataset, length=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sleeping beauty is to be mayor.’ The man went ou\n"
     ]
    }
   ],
   "source": [
    "predict_new_seq_given_seq(model, dataset, length=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
